{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd08c2dedfe5a045a9483c4a9d888731e70df5363cf8c06ebd0819183f007864c1f",
   "display_name": "Python 3.8.5 64-bit ('baseline': virtualenvwrapper)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8c2dedfe5a045a9483c4a9d888731e70df5363cf8c06ebd0819183f007864c1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import albumentations\n",
    "import timm\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from torchvision.models import resnet50,resnet18\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from facenet_pytorch import MTCNN,extract_face\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(seed=2021):\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "seeding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"UTK.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "max(data.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    img_size=256\n",
    "    max_epochs=100\n",
    "    model_name = \"resnet18\"\n",
    "    patience = [5,2]\n",
    "    factor= .1\n",
    "    folds=5\n",
    "    min_lr=1e-8\n",
    "    logger = WandbLogger(project=\"age_gend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class age_gend_dataset(Dataset):\n",
    "    def __init__(self,df,transforms):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        #self.root_dir = root_dir\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img = cv2.imread(self.df.loc[idx,'path'])\n",
    "        #print(img.shape)\n",
    "        age = self.df.loc[idx,'age']\n",
    "        gend = self.df.loc[idx,'gender']\n",
    "        assert img is not None\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)['image']\n",
    "\n",
    "        sample = {'image':img,'age':torch.Tensor([age])/116,'gender':torch.Tensor([gend])}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_Loss(nn.Module):\n",
    "    def __init__(self,task_num):\n",
    "        super(MTL_Loss,self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.log_vars = nn.Parameter(torch.zeros((self.task_num)))\n",
    "    def forward(self,pred_age,pred_gend,tar_gend,tar_age):\n",
    "        \n",
    "\n",
    "        loss0 = nn.functional.binary_cross_entropy_with_logits(pred_gend,tar_gend)\n",
    "        loss1 = nn.functional.mse_loss(pred_age,tar_age)\n",
    "\n",
    "        precision0 = torch.exp(-self.log_vars[0])\n",
    "        loss0 = precision0*loss0 + self.log_vars[0]\n",
    "\n",
    "        precision1 = torch.exp(-self.log_vars[1])\n",
    "        loss1 = precision1*loss1 + self.log_vars[1]\n",
    "\n",
    "        return loss0+loss1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl = MTL_Loss(task_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augs():\n",
    "    return albumentations.Compose([\n",
    "       albumentations.RandomResizedCrop(cfg.img_size, cfg.img_size, scale=(0.9, 1), p=1), \n",
    "       albumentations.HorizontalFlip(p=0.5),\n",
    "       albumentations.VerticalFlip(p=0.5),\n",
    "       albumentations.ShiftScaleRotate(p=0.5),\n",
    "       albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.7),\n",
    "       albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),\n",
    "       albumentations.CLAHE(clip_limit=(1,4), p=0.5),\n",
    "       albumentations.OneOf([\n",
    "           albumentations.OpticalDistortion(distort_limit=1.0),\n",
    "           albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "           albumentations.ElasticTransform(alpha=3),\n",
    "       ], p=0.2),\n",
    "       albumentations.OneOf([\n",
    "           albumentations.GaussNoise(var_limit=[10, 50]),\n",
    "           albumentations.GaussianBlur(),\n",
    "           albumentations.MotionBlur(),\n",
    "           albumentations.MedianBlur(),\n",
    "       ], p=0.2),\n",
    "      albumentations.Resize(cfg.img_size, cfg.img_size),\n",
    "      albumentations.OneOf([\n",
    "          albumentations.JpegCompression(),\n",
    "          albumentations.Downscale(scale_min=0.1, scale_max=0.15),\n",
    "      ], p=0.2),\n",
    "      albumentations.IAAPiecewiseAffine(p=0.2),\n",
    "      albumentations.IAASharpen(p=0.2),\n",
    "      albumentations.Cutout(max_h_size=int(cfg.img_size * 0.1), max_w_size=int(cfg.img_size * 0.1), num_holes=5, p=0.5),\n",
    "      albumentations.Normalize(p=1.0),\n",
    "      ToTensorV2()\n",
    "    ])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeGendModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AgeGendModel,self).__init__()\n",
    "        if 'resnet18' in cfg.model_name:\n",
    "            self.model = eval(cfg.model_name)(pretrained=False)\n",
    "            for params in self.model.parameters():\n",
    "                params.require_grad = True\n",
    "            self.model = nn.Sequential(*list(self.model.children())[:-2])\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        \n",
    "            self.clf = nn.Linear(512,1)\n",
    "            self.reg = nn.Linear(512,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print(x.shape)\n",
    "        #x = self.lin1(x)\n",
    "        #x = self.lin2(x)\n",
    "\n",
    "        gend = self.clf(x)\n",
    "        age = torch.sigmoid(self.reg(x))\n",
    "\n",
    "\n",
    "        return gend,age\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AgeGendModel()"
   ]
  },
  {
   "source": [
    "## Model Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeGendNet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(AgeGendNet,self).__init__()\n",
    "\n",
    "        self.model = AgeGendModel()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        op = optim.Adam(self.model.parameters(),lr=0.01)\n",
    "        \n",
    "        scheduler = {\n",
    "            'scheduler':optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=op,T_0=10),\n",
    "            'monitor':'val_loss',\n",
    "            'interval':'epoch',\n",
    "            'frequency':1,\n",
    "            'strict':True\n",
    "        }\n",
    "\n",
    "        self.op = op\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        return [op],[scheduler]\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        y_hat_gend,y_hat_age = self.model(batch['image'])\n",
    "        loss_tr = mtl(pred_gend=y_hat_gend,pred_age=y_hat_age,tar_gend=batch['gender'],tar_age=batch['age'])\n",
    "        #f1_tr = torchmetrics.functional.accuracy(y_hat_gend.sigmoid(),batch['gender']) \n",
    "        #mae_tr = torchmetrics.functional.mean_absolute_error(y_hat_age.relu(),batch['age'])\n",
    "        self.log(\"TrainLoss\",loss_tr,prog_bar=True,on_step=False,on_epoch=True)\n",
    "        return loss_tr\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        y_hat_gend,y_hat_age = self.model(batch['image'])\n",
    "        loss_val = mtl(pred_gend=y_hat_gend,pred_age=y_hat_age,tar_gend=batch['gender'],tar_age=batch['age'])\n",
    "        #f1_val = torchmetrics.functional.accuracy(y_hat_gend.sigmoid(),batch['gender'])\n",
    "        #mae_val = torchmetrics.functional.mean_absolute_error(y_hat_age.relu(),batch['age'])\n",
    "        self.log(\"val_loss\",loss_val,prog_bar=True,on_step=False,on_epoch=True)\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgeGendNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"UTK.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    i=0\n",
    "\n",
    "    estp = EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=cfg.patience[0])\n",
    "\n",
    "    if os.path.isdir(f\"{cfg.model_name}_{cfg.img_size}_Model_Fold-{i+1}-2\"):\n",
    "        print(\"Already Exists\")\n",
    "    else:\n",
    "        os.makedirs(f\"{cfg.model_name}_{cfg.img_size}_Model_Fold-{i+1}-2\")\n",
    "    \n",
    "\n",
    "    mod_ckpt = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    dirpath=f\"{cfg.model_name}_{cfg.img_size}_Model_Fold-{i+1}-2\",\n",
    "    filename='Resnet50_256_Checkpoint-ValLoss:{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    ")\n",
    "    trainer = pl.Trainer(gpus=1,precision=16,max_epochs=cfg.max_epochs,progress_bar_refresh_rate=30,deterministic=True,benchmark=True,callbacks=[mod_ckpt,estp],logger=cfg.logger,accumulate_grad_batches=1)\n",
    "    \n",
    "    \n",
    "    print(f\"Initializing model Fold - {i+1}/5\")\n",
    "    model = AgeGendNet()\n",
    "    print(\"*** Model Initialization Completed ***\")\n",
    "    train_recs = train_data[train_data.folds != i].reset_index(drop=True)\n",
    "    val_recs = train_data[train_data.folds == i].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = age_gend_dataset(df=train_recs,transforms=train_augs())\n",
    "    val_dataset = age_gend_dataset(df=val_recs,transforms=val_augs())\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,batch_size=32,num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=32,num_workers=4)\n",
    "\n",
    "    trainer.fit(model,train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "eGendModel | 11.2 M\n",
      "---------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n",
      "Initializing model Fold - 1/5\n",
      "*** Model Initialization Completed ***\n",
      "Epoch 0:  81%|████████  | 600/742 [02:13<00:31,  4.51it/s, loss=0.694, v_num=li8g, val_loss=1.040]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  85%|████████▍ | 630/742 [02:13<00:23,  4.71it/s, loss=0.694, v_num=li8g, val_loss=1.040]\n",
      "Epoch 0:  89%|████████▉ | 660/742 [02:14<00:16,  4.91it/s, loss=0.694, v_num=li8g, val_loss=1.040]\n",
      "Epoch 0:  93%|█████████▎| 690/742 [02:14<00:10,  5.12it/s, loss=0.694, v_num=li8g, val_loss=1.040]\n",
      "Epoch 0:  97%|█████████▋| 720/742 [02:15<00:04,  5.32it/s, loss=0.694, v_num=li8g, val_loss=1.040]\n",
      "Epoch 0: 100%|██████████| 742/742 [02:15<00:00,  5.46it/s, loss=0.68, v_num=li8g, val_loss=0.652, TrainLoss=0.728]\n",
      "Epoch 1:  81%|████████  | 600/742 [02:14<00:31,  4.45it/s, loss=0.679, v_num=li8g, val_loss=0.652, TrainLoss=0.728]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 630/742 [02:15<00:24,  4.65it/s, loss=0.679, v_num=li8g, val_loss=0.652, TrainLoss=0.728]\n",
      "Epoch 1:  89%|████████▉ | 660/742 [02:16<00:16,  4.85it/s, loss=0.679, v_num=li8g, val_loss=0.652, TrainLoss=0.728]\n",
      "Epoch 1:  93%|█████████▎| 690/742 [02:16<00:10,  5.05it/s, loss=0.679, v_num=li8g, val_loss=0.652, TrainLoss=0.728]\n",
      "Epoch 1:  97%|█████████▋| 720/742 [02:17<00:04,  5.25it/s, loss=0.679, v_num=li8g, val_loss=0.652, TrainLoss=0.728]\n",
      "Epoch 1: 100%|██████████| 742/742 [02:17<00:00,  5.39it/s, loss=0.651, v_num=li8g, val_loss=0.643, TrainLoss=0.669]\n",
      "Epoch 2:  81%|████████  | 600/742 [02:11<00:31,  4.56it/s, loss=0.637, v_num=li8g, val_loss=0.643, TrainLoss=0.669]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  85%|████████▍ | 630/742 [02:12<00:23,  4.76it/s, loss=0.637, v_num=li8g, val_loss=0.643, TrainLoss=0.669]\n",
      "Epoch 2:  89%|████████▉ | 660/742 [02:12<00:16,  4.97it/s, loss=0.637, v_num=li8g, val_loss=0.643, TrainLoss=0.669]\n",
      "Epoch 2:  93%|█████████▎| 690/742 [02:13<00:10,  5.17it/s, loss=0.637, v_num=li8g, val_loss=0.643, TrainLoss=0.669]\n",
      "Epoch 2:  97%|█████████▋| 720/742 [02:13<00:04,  5.38it/s, loss=0.637, v_num=li8g, val_loss=0.643, TrainLoss=0.669]\n",
      "Epoch 2: 100%|██████████| 742/742 [02:14<00:00,  5.52it/s, loss=0.636, v_num=li8g, val_loss=0.622, TrainLoss=0.645]\n",
      "Epoch 3:  81%|████████  | 600/742 [02:11<00:31,  4.57it/s, loss=0.613, v_num=li8g, val_loss=0.622, TrainLoss=0.645]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  85%|████████▍ | 630/742 [02:11<00:23,  4.77it/s, loss=0.613, v_num=li8g, val_loss=0.622, TrainLoss=0.645]\n",
      "Epoch 3:  89%|████████▉ | 660/742 [02:12<00:16,  4.98it/s, loss=0.613, v_num=li8g, val_loss=0.622, TrainLoss=0.645]\n",
      "Epoch 3:  93%|█████████▎| 690/742 [02:12<00:10,  5.19it/s, loss=0.613, v_num=li8g, val_loss=0.622, TrainLoss=0.645]\n",
      "Epoch 3:  97%|█████████▋| 720/742 [02:13<00:04,  5.39it/s, loss=0.613, v_num=li8g, val_loss=0.622, TrainLoss=0.645]\n",
      "Epoch 3: 100%|██████████| 742/742 [02:14<00:00,  5.54it/s, loss=0.604, v_num=li8g, val_loss=0.564, TrainLoss=0.621]\n",
      "Epoch 4:  81%|████████  | 600/742 [02:13<00:31,  4.51it/s, loss=0.591, v_num=li8g, val_loss=0.564, TrainLoss=0.621]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  85%|████████▍ | 630/742 [02:13<00:23,  4.71it/s, loss=0.591, v_num=li8g, val_loss=0.564, TrainLoss=0.621]\n",
      "Epoch 4:  89%|████████▉ | 660/742 [02:14<00:16,  4.92it/s, loss=0.591, v_num=li8g, val_loss=0.564, TrainLoss=0.621]\n",
      "Epoch 4:  93%|█████████▎| 690/742 [02:14<00:10,  5.12it/s, loss=0.591, v_num=li8g, val_loss=0.564, TrainLoss=0.621]\n",
      "Epoch 4:  97%|█████████▋| 720/742 [02:15<00:04,  5.32it/s, loss=0.591, v_num=li8g, val_loss=0.564, TrainLoss=0.621]\n",
      "Epoch 4: 100%|██████████| 742/742 [02:15<00:00,  5.46it/s, loss=0.549, v_num=li8g, val_loss=0.484, TrainLoss=0.579]\n",
      "Epoch 5:  81%|████████  | 600/742 [02:12<00:31,  4.52it/s, loss=0.524, v_num=li8g, val_loss=0.484, TrainLoss=0.579]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  85%|████████▍ | 630/742 [02:13<00:23,  4.72it/s, loss=0.524, v_num=li8g, val_loss=0.484, TrainLoss=0.579]\n",
      "Epoch 5:  89%|████████▉ | 660/742 [02:13<00:16,  4.93it/s, loss=0.524, v_num=li8g, val_loss=0.484, TrainLoss=0.579]\n",
      "Epoch 5:  93%|█████████▎| 690/742 [02:14<00:10,  5.13it/s, loss=0.524, v_num=li8g, val_loss=0.484, TrainLoss=0.579]\n",
      "Epoch 5:  97%|█████████▋| 720/742 [02:14<00:04,  5.34it/s, loss=0.524, v_num=li8g, val_loss=0.484, TrainLoss=0.579]\n",
      "Epoch 5: 100%|██████████| 742/742 [02:15<00:00,  5.48it/s, loss=0.498, v_num=li8g, val_loss=0.404, TrainLoss=0.519]\n",
      "Epoch 6:  81%|████████  | 600/742 [02:09<00:30,  4.62it/s, loss=0.514, v_num=li8g, val_loss=0.404, TrainLoss=0.519]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  85%|████████▍ | 630/742 [02:10<00:23,  4.83it/s, loss=0.514, v_num=li8g, val_loss=0.404, TrainLoss=0.519]\n",
      "Epoch 6:  89%|████████▉ | 660/742 [02:10<00:16,  5.04it/s, loss=0.514, v_num=li8g, val_loss=0.404, TrainLoss=0.519]\n",
      "Epoch 6:  93%|█████████▎| 690/742 [02:11<00:09,  5.25it/s, loss=0.514, v_num=li8g, val_loss=0.404, TrainLoss=0.519]\n",
      "Epoch 6:  97%|█████████▋| 720/742 [02:11<00:04,  5.46it/s, loss=0.514, v_num=li8g, val_loss=0.404, TrainLoss=0.519]\n",
      "Epoch 6: 100%|██████████| 742/742 [02:12<00:00,  5.60it/s, loss=0.477, v_num=li8g, val_loss=0.386, TrainLoss=0.476]\n",
      "Epoch 7:  81%|████████  | 600/742 [02:10<00:30,  4.59it/s, loss=0.458, v_num=li8g, val_loss=0.386, TrainLoss=0.476]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  85%|████████▍ | 630/742 [02:11<00:23,  4.79it/s, loss=0.458, v_num=li8g, val_loss=0.386, TrainLoss=0.476]\n",
      "Epoch 7:  89%|████████▉ | 660/742 [02:12<00:16,  5.00it/s, loss=0.458, v_num=li8g, val_loss=0.386, TrainLoss=0.476]\n",
      "Epoch 7:  93%|█████████▎| 690/742 [02:12<00:09,  5.21it/s, loss=0.458, v_num=li8g, val_loss=0.386, TrainLoss=0.476]\n",
      "Epoch 7:  97%|█████████▋| 720/742 [02:13<00:04,  5.41it/s, loss=0.458, v_num=li8g, val_loss=0.386, TrainLoss=0.476]\n",
      "Epoch 7: 100%|██████████| 742/742 [02:13<00:00,  5.56it/s, loss=0.46, v_num=li8g, val_loss=0.359, TrainLoss=0.442] \n",
      "Epoch 8:  81%|████████  | 600/742 [02:11<00:31,  4.55it/s, loss=0.464, v_num=li8g, val_loss=0.359, TrainLoss=0.442]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  85%|████████▍ | 630/742 [02:12<00:23,  4.75it/s, loss=0.464, v_num=li8g, val_loss=0.359, TrainLoss=0.442]\n",
      "Epoch 8:  89%|████████▉ | 660/742 [02:13<00:16,  4.96it/s, loss=0.464, v_num=li8g, val_loss=0.359, TrainLoss=0.442]\n",
      "Epoch 8:  93%|█████████▎| 690/742 [02:13<00:10,  5.16it/s, loss=0.464, v_num=li8g, val_loss=0.359, TrainLoss=0.442]\n",
      "Epoch 8:  97%|█████████▋| 720/742 [02:14<00:04,  5.37it/s, loss=0.464, v_num=li8g, val_loss=0.359, TrainLoss=0.442]\n",
      "Epoch 8: 100%|██████████| 742/742 [02:14<00:00,  5.51it/s, loss=0.45, v_num=li8g, val_loss=0.334, TrainLoss=0.427] \n",
      "Epoch 9:  81%|████████  | 600/742 [02:10<00:30,  4.59it/s, loss=0.415, v_num=li8g, val_loss=0.334, TrainLoss=0.427]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  85%|████████▍ | 630/742 [02:11<00:23,  4.79it/s, loss=0.415, v_num=li8g, val_loss=0.334, TrainLoss=0.427]\n",
      "Epoch 9:  89%|████████▉ | 660/742 [02:11<00:16,  5.00it/s, loss=0.415, v_num=li8g, val_loss=0.334, TrainLoss=0.427]\n",
      "Epoch 9:  93%|█████████▎| 690/742 [02:12<00:09,  5.21it/s, loss=0.415, v_num=li8g, val_loss=0.334, TrainLoss=0.427]\n",
      "Epoch 9:  97%|█████████▋| 720/742 [02:12<00:04,  5.42it/s, loss=0.415, v_num=li8g, val_loss=0.334, TrainLoss=0.427]\n",
      "Epoch 9: 100%|██████████| 742/742 [02:13<00:00,  5.56it/s, loss=0.429, v_num=li8g, val_loss=0.320, TrainLoss=0.411]\n",
      "Epoch 10:  81%|████████  | 600/742 [02:10<00:30,  4.61it/s, loss=0.498, v_num=li8g, val_loss=0.320, TrainLoss=0.411]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  85%|████████▍ | 630/742 [02:10<00:23,  4.82it/s, loss=0.498, v_num=li8g, val_loss=0.320, TrainLoss=0.411]\n",
      "Epoch 10:  89%|████████▉ | 660/742 [02:11<00:16,  5.03it/s, loss=0.498, v_num=li8g, val_loss=0.320, TrainLoss=0.411]\n",
      "Epoch 10:  93%|█████████▎| 690/742 [02:11<00:09,  5.23it/s, loss=0.498, v_num=li8g, val_loss=0.320, TrainLoss=0.411]\n",
      "Epoch 10:  97%|█████████▋| 720/742 [02:12<00:04,  5.44it/s, loss=0.498, v_num=li8g, val_loss=0.320, TrainLoss=0.411]\n",
      "Epoch 10: 100%|██████████| 742/742 [02:12<00:00,  5.59it/s, loss=0.448, v_num=li8g, val_loss=0.569, TrainLoss=0.478]\n",
      "Epoch 11:  81%|████████  | 600/742 [02:09<00:30,  4.64it/s, loss=0.489, v_num=li8g, val_loss=0.569, TrainLoss=0.478]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  85%|████████▍ | 630/742 [02:09<00:23,  4.85it/s, loss=0.489, v_num=li8g, val_loss=0.569, TrainLoss=0.478]\n",
      "Epoch 11:  89%|████████▉ | 660/742 [02:10<00:16,  5.06it/s, loss=0.489, v_num=li8g, val_loss=0.569, TrainLoss=0.478]\n",
      "Epoch 11:  93%|█████████▎| 690/742 [02:11<00:09,  5.27it/s, loss=0.489, v_num=li8g, val_loss=0.569, TrainLoss=0.478]\n",
      "Epoch 11:  97%|█████████▋| 720/742 [02:11<00:04,  5.47it/s, loss=0.489, v_num=li8g, val_loss=0.569, TrainLoss=0.478]\n",
      "Epoch 11: 100%|██████████| 742/742 [02:12<00:00,  5.62it/s, loss=0.46, v_num=li8g, val_loss=0.345, TrainLoss=0.456] \n",
      "Epoch 12:  81%|████████  | 600/742 [02:10<00:30,  4.58it/s, loss=0.455, v_num=li8g, val_loss=0.345, TrainLoss=0.456]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  85%|████████▍ | 630/742 [02:11<00:23,  4.79it/s, loss=0.455, v_num=li8g, val_loss=0.345, TrainLoss=0.456]\n",
      "Epoch 12:  89%|████████▉ | 660/742 [02:12<00:16,  5.00it/s, loss=0.455, v_num=li8g, val_loss=0.345, TrainLoss=0.456]\n",
      "Epoch 12:  93%|█████████▎| 690/742 [02:12<00:09,  5.20it/s, loss=0.455, v_num=li8g, val_loss=0.345, TrainLoss=0.456]\n",
      "Epoch 12:  97%|█████████▋| 720/742 [02:13<00:04,  5.41it/s, loss=0.455, v_num=li8g, val_loss=0.345, TrainLoss=0.456]\n",
      "Epoch 12: 100%|██████████| 742/742 [02:13<00:00,  5.55it/s, loss=0.415, v_num=li8g, val_loss=0.387, TrainLoss=0.431]\n",
      "Epoch 13:  81%|████████  | 600/742 [02:09<00:30,  4.65it/s, loss=0.424, v_num=li8g, val_loss=0.387, TrainLoss=0.431]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  85%|████████▍ | 630/742 [02:09<00:23,  4.86it/s, loss=0.424, v_num=li8g, val_loss=0.387, TrainLoss=0.431]\n",
      "Epoch 13:  89%|████████▉ | 660/742 [02:10<00:16,  5.07it/s, loss=0.424, v_num=li8g, val_loss=0.387, TrainLoss=0.431]\n",
      "Epoch 13:  93%|█████████▎| 690/742 [02:10<00:09,  5.28it/s, loss=0.424, v_num=li8g, val_loss=0.387, TrainLoss=0.431]\n",
      "Epoch 13:  97%|█████████▋| 720/742 [02:11<00:04,  5.48it/s, loss=0.424, v_num=li8g, val_loss=0.387, TrainLoss=0.431]\n",
      "Epoch 13: 100%|██████████| 742/742 [02:11<00:00,  5.63it/s, loss=0.412, v_num=li8g, val_loss=0.347, TrainLoss=0.414]\n",
      "Epoch 14:  81%|████████  | 600/742 [02:10<00:30,  4.61it/s, loss=0.399, v_num=li8g, val_loss=0.347, TrainLoss=0.414]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  85%|████████▍ | 630/742 [02:10<00:23,  4.81it/s, loss=0.399, v_num=li8g, val_loss=0.347, TrainLoss=0.414]\n",
      "Epoch 14:  89%|████████▉ | 660/742 [02:11<00:16,  5.02it/s, loss=0.399, v_num=li8g, val_loss=0.347, TrainLoss=0.414]\n",
      "Epoch 14:  93%|█████████▎| 690/742 [02:12<00:09,  5.23it/s, loss=0.399, v_num=li8g, val_loss=0.347, TrainLoss=0.414]\n",
      "Epoch 14:  97%|█████████▋| 720/742 [02:12<00:04,  5.43it/s, loss=0.399, v_num=li8g, val_loss=0.347, TrainLoss=0.414]\n",
      "Epoch 14: 100%|██████████| 742/742 [02:13<00:00,  5.58it/s, loss=0.388, v_num=li8g, val_loss=0.303, TrainLoss=0.390]\n",
      "Epoch 15:  81%|████████  | 600/742 [02:19<00:33,  4.29it/s, loss=0.387, v_num=li8g, val_loss=0.303, TrainLoss=0.390]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  85%|████████▍ | 630/742 [02:20<00:25,  4.48it/s, loss=0.387, v_num=li8g, val_loss=0.303, TrainLoss=0.390]\n",
      "Epoch 15:  89%|████████▉ | 660/742 [02:21<00:17,  4.67it/s, loss=0.387, v_num=li8g, val_loss=0.303, TrainLoss=0.390]\n",
      "Epoch 15:  93%|█████████▎| 690/742 [02:21<00:10,  4.86it/s, loss=0.387, v_num=li8g, val_loss=0.303, TrainLoss=0.390]\n",
      "Epoch 15:  97%|█████████▋| 720/742 [02:22<00:04,  5.05it/s, loss=0.387, v_num=li8g, val_loss=0.303, TrainLoss=0.390]\n",
      "Epoch 15: 100%|██████████| 742/742 [02:23<00:00,  5.19it/s, loss=0.4, v_num=li8g, val_loss=0.291, TrainLoss=0.373]  \n",
      "Epoch 16:  81%|████████  | 600/742 [02:27<00:34,  4.07it/s, loss=0.369, v_num=li8g, val_loss=0.291, TrainLoss=0.373]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  85%|████████▍ | 630/742 [02:28<00:26,  4.25it/s, loss=0.369, v_num=li8g, val_loss=0.291, TrainLoss=0.373]\n",
      "Epoch 16:  89%|████████▉ | 660/742 [02:28<00:18,  4.44it/s, loss=0.369, v_num=li8g, val_loss=0.291, TrainLoss=0.373]\n",
      "Epoch 16:  93%|█████████▎| 690/742 [02:29<00:11,  4.62it/s, loss=0.369, v_num=li8g, val_loss=0.291, TrainLoss=0.373]\n",
      "Epoch 16:  97%|█████████▋| 720/742 [02:29<00:04,  4.80it/s, loss=0.369, v_num=li8g, val_loss=0.291, TrainLoss=0.373]\n",
      "Epoch 16: 100%|██████████| 742/742 [02:30<00:00,  4.93it/s, loss=0.368, v_num=li8g, val_loss=0.282, TrainLoss=0.352]\n",
      "Epoch 17:  81%|████████  | 600/742 [02:27<00:34,  4.07it/s, loss=0.348, v_num=li8g, val_loss=0.282, TrainLoss=0.352]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  85%|████████▍ | 630/742 [02:28<00:26,  4.25it/s, loss=0.348, v_num=li8g, val_loss=0.282, TrainLoss=0.352]\n",
      "Epoch 17:  89%|████████▉ | 660/742 [02:28<00:18,  4.43it/s, loss=0.348, v_num=li8g, val_loss=0.282, TrainLoss=0.352]\n",
      "Epoch 17:  93%|█████████▎| 690/742 [02:29<00:11,  4.61it/s, loss=0.348, v_num=li8g, val_loss=0.282, TrainLoss=0.352]\n",
      "Epoch 17:  97%|█████████▋| 720/742 [02:30<00:04,  4.79it/s, loss=0.348, v_num=li8g, val_loss=0.282, TrainLoss=0.352]\n",
      "Epoch 17: 100%|██████████| 742/742 [02:30<00:00,  4.92it/s, loss=0.339, v_num=li8g, val_loss=0.257, TrainLoss=0.337]\n",
      "Epoch 18:  81%|████████  | 600/742 [02:36<00:37,  3.83it/s, loss=0.338, v_num=li8g, val_loss=0.257, TrainLoss=0.337]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  85%|████████▍ | 630/742 [02:37<00:27,  4.00it/s, loss=0.338, v_num=li8g, val_loss=0.257, TrainLoss=0.337]\n",
      "Epoch 18:  89%|████████▉ | 660/742 [02:38<00:19,  4.18it/s, loss=0.338, v_num=li8g, val_loss=0.257, TrainLoss=0.337]\n",
      "Epoch 18:  93%|█████████▎| 690/742 [02:38<00:11,  4.35it/s, loss=0.338, v_num=li8g, val_loss=0.257, TrainLoss=0.337]\n",
      "Epoch 18:  97%|█████████▋| 720/742 [02:39<00:04,  4.52it/s, loss=0.338, v_num=li8g, val_loss=0.257, TrainLoss=0.337]\n",
      "Epoch 18: 100%|██████████| 742/742 [02:39<00:00,  4.64it/s, loss=0.355, v_num=li8g, val_loss=0.250, TrainLoss=0.326]\n",
      "Epoch 19:  81%|████████  | 600/742 [02:33<00:36,  3.91it/s, loss=0.328, v_num=li8g, val_loss=0.250, TrainLoss=0.326]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  85%|████████▍ | 630/742 [02:34<00:27,  4.09it/s, loss=0.328, v_num=li8g, val_loss=0.250, TrainLoss=0.326]\n",
      "Epoch 19:  89%|████████▉ | 660/742 [02:34<00:19,  4.26it/s, loss=0.328, v_num=li8g, val_loss=0.250, TrainLoss=0.326]\n",
      "Epoch 19:  93%|█████████▎| 690/742 [02:35<00:11,  4.44it/s, loss=0.328, v_num=li8g, val_loss=0.250, TrainLoss=0.326]\n",
      "Epoch 19:  97%|█████████▋| 720/742 [02:36<00:04,  4.61it/s, loss=0.328, v_num=li8g, val_loss=0.250, TrainLoss=0.326]\n",
      "Epoch 19: 100%|██████████| 742/742 [02:36<00:00,  4.73it/s, loss=0.356, v_num=li8g, val_loss=0.250, TrainLoss=0.316]\n",
      "Epoch 20:  81%|████████  | 600/742 [02:35<00:36,  3.86it/s, loss=0.392, v_num=li8g, val_loss=0.250, TrainLoss=0.316]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  85%|████████▍ | 630/742 [02:36<00:27,  4.03it/s, loss=0.392, v_num=li8g, val_loss=0.250, TrainLoss=0.316]\n",
      "Epoch 20:  89%|████████▉ | 660/742 [02:36<00:19,  4.21it/s, loss=0.392, v_num=li8g, val_loss=0.250, TrainLoss=0.316]\n",
      "Epoch 20:  93%|█████████▎| 690/742 [02:37<00:11,  4.38it/s, loss=0.392, v_num=li8g, val_loss=0.250, TrainLoss=0.316]\n",
      "Epoch 20:  97%|█████████▋| 720/742 [02:38<00:04,  4.55it/s, loss=0.392, v_num=li8g, val_loss=0.250, TrainLoss=0.316]\n",
      "Epoch 20: 100%|██████████| 742/742 [02:38<00:00,  4.67it/s, loss=0.369, v_num=li8g, val_loss=0.329, TrainLoss=0.376]\n",
      "Epoch 21:  81%|████████  | 600/742 [02:25<00:34,  4.14it/s, loss=0.38, v_num=li8g, val_loss=0.329, TrainLoss=0.376]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  85%|████████▍ | 630/742 [02:25<00:25,  4.32it/s, loss=0.38, v_num=li8g, val_loss=0.329, TrainLoss=0.376]\n",
      "Epoch 21:  89%|████████▉ | 660/742 [02:26<00:18,  4.51it/s, loss=0.38, v_num=li8g, val_loss=0.329, TrainLoss=0.376]\n",
      "Epoch 21:  93%|█████████▎| 690/742 [02:26<00:11,  4.69it/s, loss=0.38, v_num=li8g, val_loss=0.329, TrainLoss=0.376]\n",
      "Epoch 21:  97%|█████████▋| 720/742 [02:27<00:04,  4.88it/s, loss=0.38, v_num=li8g, val_loss=0.329, TrainLoss=0.376]\n",
      "Epoch 21: 100%|██████████| 742/742 [02:28<00:00,  5.01it/s, loss=0.377, v_num=li8g, val_loss=0.327, TrainLoss=0.367]\n",
      "Epoch 22:  81%|████████  | 600/742 [02:19<00:32,  4.31it/s, loss=0.389, v_num=li8g, val_loss=0.327, TrainLoss=0.367]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  85%|████████▍ | 630/742 [02:20<00:24,  4.50it/s, loss=0.389, v_num=li8g, val_loss=0.327, TrainLoss=0.367]\n",
      "Epoch 22:  89%|████████▉ | 660/742 [02:20<00:17,  4.69it/s, loss=0.389, v_num=li8g, val_loss=0.327, TrainLoss=0.367]\n",
      "Epoch 22:  93%|█████████▎| 690/742 [02:21<00:10,  4.88it/s, loss=0.389, v_num=li8g, val_loss=0.327, TrainLoss=0.367]\n",
      "Epoch 22:  97%|█████████▋| 720/742 [02:21<00:04,  5.07it/s, loss=0.389, v_num=li8g, val_loss=0.327, TrainLoss=0.367]\n",
      "Epoch 22: 100%|██████████| 742/742 [02:22<00:00,  5.21it/s, loss=0.381, v_num=li8g, val_loss=0.266, TrainLoss=0.356]\n",
      "Epoch 23:  81%|████████  | 600/742 [02:21<00:33,  4.25it/s, loss=0.371, v_num=li8g, val_loss=0.266, TrainLoss=0.356]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  85%|████████▍ | 630/742 [02:21<00:25,  4.44it/s, loss=0.371, v_num=li8g, val_loss=0.266, TrainLoss=0.356]\n",
      "Epoch 23:  89%|████████▉ | 660/742 [02:22<00:17,  4.63it/s, loss=0.371, v_num=li8g, val_loss=0.266, TrainLoss=0.356]\n",
      "Epoch 23:  93%|█████████▎| 690/742 [02:23<00:10,  4.82it/s, loss=0.371, v_num=li8g, val_loss=0.266, TrainLoss=0.356]\n",
      "Epoch 23:  97%|█████████▋| 720/742 [02:23<00:04,  5.01it/s, loss=0.371, v_num=li8g, val_loss=0.266, TrainLoss=0.356]\n",
      "Epoch 23: 100%|██████████| 742/742 [02:24<00:00,  5.14it/s, loss=0.349, v_num=li8g, val_loss=0.255, TrainLoss=0.344]\n",
      "Epoch 24:  81%|████████  | 600/742 [02:21<00:33,  4.23it/s, loss=0.365, v_num=li8g, val_loss=0.255, TrainLoss=0.344]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  85%|████████▍ | 630/742 [02:22<00:25,  4.41it/s, loss=0.365, v_num=li8g, val_loss=0.255, TrainLoss=0.344]\n",
      "Epoch 24:  89%|████████▉ | 660/742 [02:23<00:17,  4.61it/s, loss=0.365, v_num=li8g, val_loss=0.255, TrainLoss=0.344]\n",
      "Epoch 24:  93%|█████████▎| 690/742 [02:23<00:10,  4.80it/s, loss=0.365, v_num=li8g, val_loss=0.255, TrainLoss=0.344]\n",
      "Epoch 24:  97%|█████████▋| 720/742 [02:24<00:04,  4.98it/s, loss=0.365, v_num=li8g, val_loss=0.255, TrainLoss=0.344]\n",
      "Epoch 24: 100%|██████████| 742/742 [02:25<00:00,  5.12it/s, loss=0.355, v_num=li8g, val_loss=0.272, TrainLoss=0.330]\n",
      "Epoch 24: 100%|██████████| 742/742 [02:25<00:00,  5.12it/s, loss=0.355, v_num=li8g, val_loss=0.272, TrainLoss=0.330]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "928,  1447.9056, 15595.3271,\n",
       "                       1399.6440, 12397.3584,  4446.0747,  4913.3857, 11369.0117,   972.9417,\n",
       "                        384.1671,  6722.8203,  8443.7656,   829.0696,   928.3211,  1049.8445,\n",
       "                        255.9720, 10292.6934, 15058.1182,  2261.2695,  2094.4441,  5347.5503,\n",
       "                       4247.1079,   210.3136,  2566.8687,  1486.0464,  3581.6904,   601.6384,\n",
       "                      11875.2842,   196.8144,  5330.7832,   720.2455,  1410.9438,  2394.3589,\n",
       "                       1500.3696,  4117.3574,   698.1847,  9151.6299,  6040.6343,  4428.8623,\n",
       "                       3508.2937,  6104.7402,  2192.2209,  8163.5342,   448.6442, 16883.4863,\n",
       "                       3584.3674,  6412.1675,  3056.7168,  4067.2166,  4326.5020,  1516.6954,\n",
       "                        506.0337,  8971.5098,  7985.8037,  3375.4756,  5400.2642,  8261.2627,\n",
       "                      11316.9453, 19741.1152,  2599.9685,  8815.8770,  1531.7903,  7633.0493,\n",
       "                       3771.5691,   325.2081, 10296.9912,  1041.4542,  3888.5398,   313.8314,\n",
       "                      10176.4648,   769.8734,  6506.1270,  3926.0115,  3665.8115,  3678.0181,\n",
       "                       4920.5444,   704.5721, 11902.4229,  2027.4095,  1321.4285,  4712.1484,\n",
       "                       5966.0068,  1648.0170,  1733.7017,   742.9099,  2706.2332,  2930.6880,\n",
       "                       2236.9333,  1988.9333,   942.8500,  6793.5176,  9099.4023,  2606.8308,\n",
       "                        603.5369,  1117.5583,  6930.1211,  5842.2998,   509.5362,  1416.2963,\n",
       "                        712.1737,  7968.0322], device='cuda:0')),\n",
       "             ('model.model.7.1.bn2.num_batches_tracked',\n",
       "              tensor(11267, device='cuda:0')),\n",
       "             ('model.clf.weight',\n",
       "              tensor([[ 3.4613e-01,  5.8903e-02,  2.9608e-01, -2.8096e-01,  7.5943e-01,\n",
       "                       -3.1656e-02,  1.4713e-02, -8.4932e-02,  5.6084e-02, -3.7641e-01,\n",
       "                       -4.1930e-03, -3.3589e-01,  3.5070e-01, -3.8705e-03, -1.4252e-01,\n",
       "                       -3.4743e-01, -5.9039e-01,  1.7169e-02, -1.0762e-02,  2.6406e-02,\n",
       "                       -5.6853e-02, -1.0391e+00,  1.4134e-02, -2.4452e-04, -2.2780e-01,\n",
       "                       -6.8897e-03,  2.0383e-02, -2.3383e-01, -4.0320e-01,  5.1752e-02,\n",
       "                        3.6384e-01, -2.1507e-02,  6.5130e-02, -6.1265e-01,  1.4544e-01,\n",
       "                        4.6391e-02, -2.2540e-01,  2.8609e-02,  2.1131e-02, -1.2365e-01,\n",
       "                       -2.9522e-01, -3.1575e-03, -7.4562e-03,  1.8225e-04, -3.3336e-01,\n",
       "                       -8.3126e-02, -7.7013e-01,  6.3816e-03, -1.0925e-02, -1.6274e-01,\n",
       "                        2.2560e-02,  1.9330e-01,  1.1679e-03, -1.4225e-01, -4.5782e-01,\n",
       "                       -4.3954e-03, -2.4116e-01, -4.6845e-01,  1.0161e-02,  2.7096e-03,\n",
       "                        2.5354e-02, -1.2054e-02, -3.8201e-02,  2.7057e-01,  1.4781e-01,\n",
       "                       -9.8240e-02, -1.7815e-02, -2.3601e-02,  1.4688e-01,  7.1317e-01,\n",
       "                        9.9018e-03,  5.7876e-02, -2.3240e-01,  5.9543e-04,  7.2827e-02,\n",
       "                        1.3234e-02, -2.0197e-01,  4.5469e-01, -4.4488e-02,  5.0980e-01,\n",
       "                        6.3175e-03,  1.5606e-02, -3.8853e-02, -6.4668e-02,  1.1790e-02,\n",
       "                       -2.5537e-01, -1.6884e-02,  5.6704e-02,  8.2632e-03,  3.9592e-01,\n",
       "                       -8.3858e-03, -3.2827e-02,  7.9942e-02,  5.8334e-01,  1.4609e-02,\n",
       "                       -3.7666e-02,  3.4688e-03,  3.3777e-02, -2.9380e-02,  1.4229e-02,\n",
       "                       -1.4634e-01, -3.3483e-01, -3.4623e-01,  1.3553e-01,  5.0821e-02,\n",
       "                        9.3126e-03,  2.3983e-01,  1.2745e-01, -3.9374e-01, -2.3975e-01,\n",
       "                        1.5046e-01, -1.6669e-01,  1.0235e-01, -4.8256e-02,  3.5626e-02,\n",
       "                        1.1479e-01,  9.0940e-03,  8.0125e-04, -8.7776e-03, -9.6068e-01,\n",
       "                       -7.1755e-03,  2.2588e-01, -2.0782e-02, -5.0437e-02, -1.3353e-01,\n",
       "                        2.8112e-01,  3.3071e-01,  5.9053e-02, -4.5467e-02,  7.5149e-03,\n",
       "                       -1.4385e-01, -1.3440e-01,  5.5148e-02, -1.4263e-01,  4.5171e-01,\n",
       "                        6.7331e-03,  7.2091e-02,  2.8293e-01,  7.5521e-02, -2.6167e-02,\n",
       "                       -5.1077e-02,  4.9550e-02,  3.6716e-01,  2.9094e-02, -2.6253e-02,\n",
       "                        2.7163e-01, -9.3749e-02,  6.0175e-02, -7.6058e-03,  2.9664e-01,\n",
       "                        1.5575e-01, -2.0150e-02,  9.7416e-02,  1.0171e-01, -2.2551e-01,\n",
       "                       -2.6592e-03, -7.3280e-02,  5.4968e-02,  7.9077e-02, -9.2019e-02,\n",
       "                        1.8971e-01,  1.1054e-01, -5.2368e-01,  7.0858e-02, -6.0830e-02,\n",
       "                       -2.6748e-02,  4.7139e-01, -3.8533e-02, -1.2374e-02, -1.0790e-03,\n",
       "                        5.9595e-03,  1.3463e-02, -3.2890e-02, -1.9665e-03,  4.8084e-01,\n",
       "                       -2.9258e-01, -1.5070e-01, -1.2328e-01,  2.6322e-02, -5.9010e-03,\n",
       "                       -3.8507e-02,  6.3778e-03,  5.0765e-01,  1.8989e-01,  6.3238e-01,\n",
       "                       -8.2110e-03,  4.8710e-02,  3.6994e-03, -6.8168e-01,  5.2559e-01,\n",
       "                        4.0507e-02,  7.7530e-02,  2.5114e-01, -2.1920e-02, -3.0851e-02,\n",
       "                        5.2824e-02, -3.3312e-01,  2.4063e-02, -3.7168e-03,  9.7245e-03,\n",
       "                       -2.0902e-01, -5.7618e-02, -4.7064e-02,  5.0411e-01, -3.7040e-02,\n",
       "                       -1.2355e-01, -9.3169e-01,  5.8474e-01,  4.9428e-03, -6.1580e-02,\n",
       "                       -5.4715e-02,  1.5807e-02, -5.6924e-02,  1.4533e-01,  3.8901e-02,\n",
       "                        8.7285e-04,  1.7505e-02,  5.5571e-01, -3.8298e-02, -3.6497e-02,\n",
       "                       -8.4753e-02,  2.7521e-02, -2.8169e-02, -5.7834e-01, -4.1631e-02,\n",
       "                       -2.6487e-01, -2.6966e-02, -6.1762e-02, -2.0688e-01, -2.4192e-01,\n",
       "                       -2.0267e-02, -6.0502e-01,  5.0804e-01, -5.8486e-02, -2.6199e-01,\n",
       "                       -9.0354e-02,  4.7352e-02,  3.2912e-02,  3.5305e-02,  2.0401e-02,\n",
       "                        4.3529e-02,  7.0362e-02, -6.2697e-01,  1.2718e-02, -3.8376e-01,\n",
       "                       -2.3843e-01,  3.5348e-02, -2.4978e-02, -1.0135e-02, -2.2348e-01,\n",
       "                       -4.3345e-02,  1.1884e-01, -3.9493e-02, -3.0648e-02, -5.4515e-04,\n",
       "                       -5.1792e-01,  8.2643e-02,  1.6942e-03,  2.0204e-02, -2.2468e-02,\n",
       "                       -7.5091e-03, -4.7577e-02,  1.3347e-01,  1.6493e-02, -1.2760e-01,\n",
       "                       -1.8923e-02, -1.8575e-01, -4.9152e-02, -1.3953e-03,  3.8929e-02,\n",
       "                       -4.0782e-02,  4.2183e-01,  1.8613e-02,  2.1613e-03,  2.8300e-02,\n",
       "                       -4.9595e-02, -7.3366e-02, -2.7457e-01,  5.6234e-01,  8.7430e-03,\n",
       "                       -3.5925e-02,  4.6749e-03, -3.5669e-01, -2.7366e-02,  6.9736e-02,\n",
       "                        3.4867e-01,  2.6908e-01,  6.0583e-02, -5.3176e-01, -2.5394e-01,\n",
       "                       -3.9074e-01,  5.1947e-03, -5.6165e-02, -2.9485e-02, -2.2841e-02,\n",
       "                        8.4234e-03, -4.4743e-02, -1.9569e-01,  3.1026e-02, -4.5698e-01,\n",
       "                       -2.3810e-02, -6.4790e-02, -1.1750e-01, -5.0781e-02, -3.5201e-03,\n",
       "                       -3.6333e-01, -2.5077e-02, -4.4570e-03, -8.6435e-02, -1.7415e-02,\n",
       "                       -4.8193e-01, -4.4090e-01,  6.7184e-01, -5.2949e-01,  4.6639e-03,\n",
       "                        1.8763e-01, -2.0330e-02,  1.3227e-01,  2.2433e-02,  5.6278e-01,\n",
       "                        1.2246e-02, -3.4856e-01, -2.7440e-01, -6.0005e-03,  1.1606e-01,\n",
       "                        1.6199e-01, -7.7293e-03, -5.8076e-02,  2.7526e-01, -1.3603e-01,\n",
       "                        5.1006e-02, -9.6870e-03,  5.4480e-01,  2.7664e-01,  3.3181e-03,\n",
       "                        4.1661e-02, -2.8044e-02,  1.2111e-02, -4.1889e-02, -1.1533e-01,\n",
       "                       -4.7654e-02, -4.8834e-02, -1.8220e-02,  9.7579e-02, -3.6070e-01,\n",
       "                       -2.4227e-01,  5.5117e-02,  1.9933e-02, -2.4465e-01, -7.5518e-02,\n",
       "                       -1.6483e-02,  4.4417e-02,  7.1139e-01, -3.9880e-03, -5.8426e-01,\n",
       "                       -6.9531e-02,  1.8851e-01,  4.6859e-02,  6.4786e-02, -7.9985e-02,\n",
       "                        2.8018e-01, -1.5880e-01, -8.8423e-03, -2.3698e-01, -3.1600e-03,\n",
       "                        1.5005e-01,  8.9332e-02,  9.5606e-03,  6.9707e-01, -2.5602e-01,\n",
       "                       -6.9318e-03,  1.4946e-02,  1.1138e-01, -2.2789e-02, -1.5232e-01,\n",
       "                       -1.1805e-01,  1.4488e-02,  4.9791e-02, -4.7896e-03, -1.4022e-01,\n",
       "                        8.6993e-04,  1.2646e-01, -9.8597e-04,  5.4579e-02,  7.7013e-02,\n",
       "                        1.7279e-01, -3.1119e-02,  2.3824e-01,  4.9471e-02, -7.1326e-03,\n",
       "                       -1.1284e-02, -9.4072e-03, -1.0183e+00, -4.2044e-03,  2.0958e-03,\n",
       "                       -2.3160e-01,  9.2312e-02, -3.4314e-02, -2.2084e-02,  2.2556e-02,\n",
       "                       -9.2231e-02,  7.7693e-02,  1.6621e-02, -6.8646e-02, -1.3621e-01,\n",
       "                       -1.2627e-02, -4.4604e-02,  1.6985e-02, -5.2105e-02, -2.7242e-02,\n",
       "                        1.9837e-02, -9.7261e-04,  5.7675e-03, -2.4063e-01,  2.6195e-03,\n",
       "                       -2.4765e-01,  8.7238e-02,  3.1598e-01, -9.4856e-01, -7.9099e-03,\n",
       "                       -2.5549e-02, -5.5544e-02,  2.1097e-01, -3.7275e-02, -4.9178e-02,\n",
       "                        4.4674e-02,  1.3166e-03, -2.7357e-01,  1.8137e-01,  7.5166e-03,\n",
       "                       -8.5077e-02, -1.9298e-01,  3.7154e-02,  2.3008e-02,  7.0685e-02,\n",
       "                        1.5983e-01,  1.3819e-01, -8.4572e-02, -1.5851e-01,  6.6369e-02,\n",
       "                        2.2704e-01,  1.9099e-02,  1.9410e-02, -1.3808e-01,  2.0103e-02,\n",
       "                        2.8421e-02,  2.1443e-03,  4.3407e-01,  1.6444e-01, -7.6282e-02,\n",
       "                        4.1189e-02, -2.3967e-02, -5.8952e-02,  2.6380e-01, -3.2379e-02,\n",
       "                        1.4687e-01,  1.4358e-01,  1.4271e-01,  3.8844e-02, -6.0416e-02,\n",
       "                        4.9837e-01, -4.6328e-02,  6.0777e-03,  1.6475e-01, -1.5273e-01,\n",
       "                       -8.7960e-02, -2.5492e-02, -7.2195e-02,  3.8460e-03, -2.4733e-01,\n",
       "                       -1.6718e-02,  2.4472e-02,  1.8638e-03, -3.5971e-02, -1.3328e-01,\n",
       "                       -3.0746e-03,  4.2929e-01, -2.0607e-02,  4.9949e-01,  7.8674e-03,\n",
       "                        5.2904e-01,  1.1258e-02,  1.7947e-01,  2.9624e-01, -8.7329e-02,\n",
       "                       -8.6540e-02,  4.7697e-04, -5.3920e-02,  6.0490e-01,  3.3630e-01,\n",
       "                       -9.8898e-03,  6.0278e-02,  6.1181e-02,  9.2898e-04, -6.7369e-02,\n",
       "                        2.2409e-02, -3.4234e-02, -2.0351e-03, -1.3401e-02,  6.8731e-03,\n",
       "                       -4.5789e-02,  1.5538e-01,  1.2141e-01, -6.3766e-02,  5.2096e-02,\n",
       "                       -5.3590e-03, -4.5712e-02, -5.8336e-01,  1.0893e-02,  9.5959e-02,\n",
       "                       -1.1625e-02, -1.2935e-01]], device='cuda:0')),\n",
       "             ('model.clf.bias', tensor([-0.1267], device='cuda:0')),\n",
       "             ('model.reg.weight',\n",
       "              tensor([[ 4.3730e-01,  3.2530e-02,  2.8355e-02, -9.6460e-02, -4.4892e-01,\n",
       "                       -1.6119e-01,  2.2798e-02,  1.7912e-01,  2.1979e-01,  4.1202e-02,\n",
       "                       -1.9360e-01, -3.8294e-01,  3.8256e-01, -2.5932e-02,  9.2153e-02,\n",
       "                       -5.3666e-01, -2.1789e-01,  5.1536e-02,  5.3787e-02, -8.7512e-02,\n",
       "                       -5.8839e-01, -8.0088e-01,  1.5728e-02,  3.1501e-02,  1.2772e-01,\n",
       "                        5.3884e-02,  1.6429e-01,  3.6053e-01,  2.4825e-01,  2.0418e-01,\n",
       "                        1.4528e-01, -5.9289e-02, -9.1096e-03,  2.2581e-01, -3.2833e-01,\n",
       "                        3.1771e-01, -1.5220e-01, -5.2376e-02, -7.3728e-03,  3.4990e-01,\n",
       "                       -1.5473e+00,  5.6638e-02, -3.9565e-02,  2.4320e-01, -2.1170e-01,\n",
       "                       -7.8718e-02, -3.4263e-01, -1.9338e-01, -1.8390e-02, -5.0628e-01,\n",
       "                       -4.3207e-01,  1.0548e-01, -4.0177e-02,  2.0264e-01,  2.2993e-01,\n",
       "                       -4.7581e-03, -2.3761e-01, -3.1003e-01, -4.9713e-02,  2.4735e-02,\n",
       "                       -4.9044e-02, -1.5190e-04, -1.3241e-01,  1.7898e-02, -5.7588e-01,\n",
       "                        6.3662e-01, -1.4008e-01,  4.0199e-02, -8.7629e-02,  1.0092e-01,\n",
       "                        7.1130e-02, -7.6925e-02, -1.0010e+00, -1.2426e-02, -1.3097e+00,\n",
       "                       -3.1610e-02,  7.2535e-02,  7.7056e-02,  5.2800e-02,  1.9908e-01,\n",
       "                       -1.2640e-02, -2.7455e-03,  4.7930e-01, -1.0161e+00,  3.4640e-02,\n",
       "                       -4.0388e-01, -1.9739e-01,  3.3124e-01,  3.9919e-03, -1.6065e-01,\n",
       "                        1.6941e-02, -2.9674e-01,  1.4785e-01, -4.3211e-01,  2.3522e-02,\n",
       "                       -2.6504e-01, -2.0709e-02,  9.6783e-03, -3.8405e-02,  2.0685e-02,\n",
       "                        8.7720e-01,  1.2352e-01,  5.5690e-01,  4.3729e-01, -4.6013e-02,\n",
       "                       -4.8423e-02,  3.5682e-01, -2.9489e-01,  5.4271e-02,  1.2899e-01,\n",
       "                        8.3039e-02, -3.2902e-01, -3.7662e-01, -1.1970e-01,  1.1269e-02,\n",
       "                       -2.2642e-01, -6.7299e-02,  5.4366e-03,  1.7359e-01, -3.0802e-01,\n",
       "                        8.3496e-02, -1.0818e-01, -5.5737e-02, -3.8500e-02, -1.4055e+00,\n",
       "                        3.2560e-01, -1.5315e-01,  4.1325e-01,  1.1073e-01, -1.4035e-01,\n",
       "                        2.9053e-02, -2.5650e-01, -1.2677e-03,  6.8636e-02, -4.3650e-01,\n",
       "                        2.7236e-02,  1.2886e-01, -2.3574e-01,  4.4394e-02, -4.5547e-02,\n",
       "                        1.6235e-01,  5.2348e-02,  3.4311e-01, -1.0256e-01, -3.0577e-02,\n",
       "                        3.9933e-01,  2.9858e-01,  3.2947e-01, -1.1968e-01,  2.9453e-02,\n",
       "                       -2.5912e-01, -1.3023e-01, -4.0966e-01,  6.0231e-01,  3.7601e-02,\n",
       "                        7.7089e-03, -7.8688e-02,  5.7778e-01,  2.1257e-01, -1.1559e+00,\n",
       "                        1.0941e+00, -7.5219e-02,  1.0623e-02,  1.7860e-01, -4.1811e-02,\n",
       "                        2.2869e-01,  2.3905e-01,  2.5594e-01,  8.6507e-04, -1.0645e-02,\n",
       "                       -8.1217e-02, -5.9280e-02,  1.3698e-01,  1.3590e-01, -7.7641e-02,\n",
       "                        2.0665e-01, -3.0229e-01,  3.2932e-01,  7.2587e-02, -9.4899e-02,\n",
       "                       -2.8619e-02, -2.4022e-02, -4.6728e-02,  4.6796e-01,  1.2695e-01,\n",
       "                       -6.7512e-02,  3.5125e-02,  1.6188e-01,  7.0495e-01, -7.0912e-03,\n",
       "                       -4.6819e-04, -1.7278e-01, -5.7217e-02,  3.0202e-01,  2.8970e-01,\n",
       "                        1.5889e-01,  2.1049e-01,  2.1346e-01, -1.2212e-02, -1.2588e-02,\n",
       "                        5.9574e-02,  2.2466e-01,  2.1995e-03,  7.8301e-02,  2.2151e-01,\n",
       "                       -2.0630e-01, -2.3934e-01, -3.9934e-01, -1.3505e-02,  1.3895e-01,\n",
       "                       -4.6368e-01, -4.7764e-02, -1.5009e-02,  4.3490e-01, -3.0965e-01,\n",
       "                       -7.7487e-02, -2.6554e-01, -1.0527e-01, -4.7576e-02, -1.2142e-01,\n",
       "                        2.0908e-01,  1.3858e-02,  3.3093e-02,  4.6355e-02,  9.2212e-02,\n",
       "                        8.2346e-02,  1.7267e-01,  1.9021e-01, -4.9763e-01,  1.6187e-01,\n",
       "                        3.3631e-01,  1.4455e-01,  2.0697e-01,  2.6886e-03,  1.2370e-01,\n",
       "                       -4.3225e-02,  3.2386e-02, -6.7629e-01,  1.4572e-01, -2.1044e-01,\n",
       "                        3.2685e-02,  1.4285e-01,  1.2026e-01, -4.4082e-01,  1.5211e-02,\n",
       "                       -5.4128e-01,  9.0518e-02,  6.4120e-02, -2.4901e-02, -6.0580e-03,\n",
       "                       -3.3198e-02,  3.9209e-01,  4.3459e-02,  1.3502e-01,  1.9570e-02,\n",
       "                        4.4718e-01,  8.2749e-02,  1.1460e-02,  1.2316e-02, -4.8496e-02,\n",
       "                        2.4707e-02,  8.4194e-02,  8.4382e-02, -3.0254e-01, -9.8703e-01,\n",
       "                        1.2561e-02,  1.5211e-01,  1.5182e-01, -3.2092e-02, -1.0061e-01,\n",
       "                       -3.0038e-02,  1.3898e-01,  4.8924e-03,  6.0532e-02,  5.0593e-02,\n",
       "                       -8.6166e-02,  1.0604e-01,  8.7645e-03,  1.2877e-01,  3.3133e-03,\n",
       "                        2.5194e-01,  1.8807e-02, -2.4794e-01,  2.3523e-02,  2.3824e-01,\n",
       "                       -1.9387e-01,  5.0078e-01,  7.2742e-02,  1.8282e-01,  3.3746e-01,\n",
       "                       -3.7654e-01,  3.3511e-02,  3.5648e-01, -1.4065e-01,  3.1456e-02,\n",
       "                       -2.1668e-01, -2.2210e-01,  1.4352e-01,  1.7491e-01, -2.7505e-01,\n",
       "                        4.7881e-02,  3.1561e-01,  1.7447e-01,  2.6267e-01, -5.7190e-02,\n",
       "                        4.5507e-02, -1.0112e-01, -1.2198e-01, -1.8533e-02, -2.4797e-01,\n",
       "                        1.3525e-01, -1.7860e-01,  6.1161e-02,  4.1276e-01,  5.1817e-03,\n",
       "                       -6.6252e-02,  1.8625e-03, -7.0311e-02,  3.3286e-01,  2.4719e-01,\n",
       "                        2.6249e-01, -5.9896e-01, -7.1320e-01,  2.9779e-02,  5.1877e-01,\n",
       "                       -4.4272e-02, -1.4633e-01,  3.8403e-01,  6.7571e-01, -1.1232e-01,\n",
       "                        6.4996e-02,  4.6866e-02, -3.9537e-01,  2.6214e-01,  3.6036e-01,\n",
       "                        1.9797e-01,  1.3830e-02,  2.3991e-02, -1.3633e-01, -1.4635e+00,\n",
       "                        9.6523e-02,  2.9604e-02,  5.7991e-02,  1.1934e-01,  2.8217e-01,\n",
       "                        1.1579e-01, -8.2763e-02,  2.1395e-01, -1.0665e+00, -5.8078e-01,\n",
       "                       -2.1150e-02,  2.3412e-01,  1.8871e-01,  1.1881e-02,  7.5586e-02,\n",
       "                        1.7829e-01,  5.2864e-01,  2.6726e-01, -1.2850e-02, -1.3336e+00,\n",
       "                       -4.1973e-01,  3.6775e-01,  2.0556e-02,  1.1712e-01, -4.1827e-02,\n",
       "                        5.1632e-01, -1.5076e-01,  5.5568e-02,  1.2825e-01,  2.2803e-01,\n",
       "                       -5.2771e-04, -3.5175e-02,  3.3573e-02,  1.1017e-01, -1.7268e-01,\n",
       "                       -4.5160e-01,  7.6937e-02,  3.7468e-01,  9.6290e-02, -1.1142e-01,\n",
       "                       -4.4060e-02,  7.9106e-02,  9.1136e-03, -1.1340e-01,  1.5376e-01,\n",
       "                       -1.2502e-01,  5.2787e-02,  2.9925e-02, -7.6723e-02,  2.7771e-02,\n",
       "                       -1.3769e-02, -9.0214e-04, -5.9493e-03,  1.0555e-02,  1.0780e-01,\n",
       "                        1.2213e-01,  1.7611e-01, -5.1029e-02,  3.8487e-01, -1.4006e+00,\n",
       "                       -1.2622e-01,  1.7098e-02,  8.1305e-02, -2.0042e-02, -4.7261e-01,\n",
       "                        6.3818e-02,  1.9364e-01,  2.3207e-02, -7.1966e-02,  7.0120e-02,\n",
       "                        2.5371e-02, -5.4240e-03,  8.2826e-02, -4.5190e-01, -2.2901e-02,\n",
       "                       -2.9974e-01, -1.8709e-01, -5.4255e-01,  1.7974e-01, -5.5286e-02,\n",
       "                        7.0211e-02, -1.4674e-01,  1.1093e-01, -4.1511e-02,  9.5317e-03,\n",
       "                       -2.0360e-01,  6.2696e-04, -5.2652e-02, -8.2690e-01,  1.4445e-01,\n",
       "                        3.5442e-01,  1.7722e-01, -9.3435e-02, -3.7755e-02,  1.4958e-01,\n",
       "                        5.0477e-02,  9.0246e-02,  1.3996e-01, -1.5326e+00,  4.3034e-02,\n",
       "                       -9.2055e-02,  1.6011e-02,  4.9701e-02,  3.0300e-01,  3.5983e-02,\n",
       "                        5.5320e-01, -7.1572e-02, -5.1327e-02, -1.8430e-01,  1.5526e-01,\n",
       "                       -2.7787e-02,  2.8420e-01, -1.9882e-01,  3.9095e-02, -2.6834e-02,\n",
       "                        1.4872e-01, -1.8577e-01,  1.1615e-01, -6.1637e-01, -1.3951e-01,\n",
       "                        4.5573e-02,  5.8802e-03,  3.0380e-02,  8.1352e-01, -7.9784e-01,\n",
       "                       -1.7560e-01,  9.4105e-02, -1.4330e+00,  2.9216e-01, -1.2916e+00,\n",
       "                       -1.4430e-01,  1.3965e-01,  5.9647e-02, -1.8344e-01,  2.8282e-01,\n",
       "                        2.1997e-02, -1.9931e-01, -8.7016e-02, -3.6987e-01, -5.6364e-02,\n",
       "                        5.6634e-01, -2.2447e-02,  2.1155e-01, -1.1385e-01, -1.0625e-01,\n",
       "                        6.6741e-02, -2.7723e-02,  4.9686e-02, -1.7759e-01, -1.3244e-01,\n",
       "                       -4.4267e-02, -4.2820e-02,  1.3003e-01,  2.1345e-02, -7.4521e-02,\n",
       "                       -5.8283e-03,  1.2435e-01, -2.4106e-02,  2.1303e-01,  9.3633e-02,\n",
       "                       -2.0903e-02,  4.1612e-01,  1.9971e-02, -2.1463e-01, -9.8051e-02,\n",
       "                        1.0466e-01,  1.7907e-01,  1.4941e-02,  9.1133e-03,  3.5793e-02,\n",
       "                       -6.2746e-03,  5.9482e-02]], device='cuda:0')),\n",
       "             ('model.reg.bias', tensor([0.6638], device='cuda:0'))])"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-2bde0e687a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmtcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/baseline/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py\u001b[0m in \u001b[0;36mextract_face\u001b[0;34m(img, box, image_size, margin, save_path)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \"\"\"\n\u001b[1;32m    358\u001b[0m     margin = [\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mmargin\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mmargin\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     ]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "img = cv2.imread(\"/home/lustbeast/office/Tamannaah_at_an_event_in_Cochin,_July_2018.jpg\")\n",
    "mtcnn = MTCNN(image_size=256,device=\"cuda:0\")\n",
    "boxes,prob = mtcnn.detect(img)\n",
    "img = extract_face(img,box=bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2742]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "torch.sigmoid(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.7519]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "out[1]"
   ]
  }
 ]
}